import logging
import sys
import os
import json
import threading
from logging import Logger, StreamHandler
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime
from pythonjsonlogger import jsonlogger
from typing import Literal, Optional

# å…¨å±€å…±äº«çš„é”™è¯¯handlerï¼Œç”¨äºæ‰€æœ‰answer logger
_shared_error_handler = None
_handler_lock = threading.Lock()


class ColoredStructuredFormatter(logging.Formatter):
    """å¸¦é¢œè‰²çš„ç»“æ„åŒ–æ—¥å¿—æ ¼å¼åŒ–å™¨"""

    COLORS = {
        'DEBUG': '\033[36m',  # Cyan
        'INFO': '\033[32m',  # Green
        'WARNING': '\033[33m',  # Yellow
        'ERROR': '\033[31m',  # Red
        'RESET': '\033[0m'
    }

    def format(self, record):
        # å¯¹äºéªŒè¯æ“ä½œï¼Œä½¿ç”¨ç‰¹æ®Šæ ¼å¼
        if hasattr(record, 'op_id'):
            op_id = record.op_id
            level_color = self.COLORS.get(record.levelname, '')
            reset = self.COLORS['RESET']

            # æ„å»ºä¸»æ¶ˆæ¯ - ç§»é™¤é‡å¤çš„levelname
            msg_parts = [
                f"{level_color}[{op_id}]{reset}"
            ]

            # æ·»åŠ èŠ‚ç‚¹ä¿¡æ¯
            if hasattr(record, 'node_id') and record.node_id:
                msg_parts.append(f"Node({record.node_id})")

            # æ·»åŠ éªŒè¯ç±»å‹
            if hasattr(record, 'verify_type'):
                msg_parts.append(f"<{record.verify_type}>")

            # æ·»åŠ ä¸»æ¶ˆæ¯
            msg_parts.append(record.getMessage())

            # æ„å»ºè¯¦ç»†ä¿¡æ¯ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
            details = []

            if hasattr(record, 'node_desc') and record.node_desc:
                details.append(f"  ğŸ“‹ Description: {record.node_desc}")

            if hasattr(record, 'url') and record.url:
                details.append(f"  ğŸ”— URL: {record.url}")

            if hasattr(record, 'claim_preview'):
                details.append(f"  ğŸ’¬ Claim: {record.claim_preview}")

            if hasattr(record, 'reasoning') and record.reasoning:
                reasoning = record.reasoning
                # if len(reasoning) > 200:
                #     reasoning = reasoning[:200] + "..."
                details.append(f"  ğŸ’­ Reasoning: {reasoning}")

            if hasattr(record, 'result'):
                result_str = "âœ… PASS" if record.result else "âŒ FAIL"
                details.append(f"  ğŸ“Š Result: {result_str}")

            # ç»„åˆæ‰€æœ‰éƒ¨åˆ†
            full_msg = " ".join(msg_parts)
            if details:
                full_msg += "\n" + "\n".join(details)

            return full_msg

        # å¯¹äºå…¶ä»–æ—¥å¿—ï¼Œä½¿ç”¨æ ‡å‡†æ ¼å¼ - åªåœ¨ERRORæ—¶æ˜¾ç¤ºçº§åˆ«
        level_indicator = ""
        if record.levelname == 'ERROR':
            level_indicator = f"{self.COLORS['ERROR']}[ERROR]{self.COLORS['RESET']} "
        elif record.levelname == 'WARNING':
            level_indicator = f"{self.COLORS['WARNING']}[WARN]{self.COLORS['RESET']} "

        return f"{level_indicator}{record.getMessage()}"


class ErrorWithContextFormatter(logging.Formatter):
    """ä¸“é—¨ç”¨äºé”™è¯¯çš„æ ¼å¼åŒ–å™¨ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""

    COLORS = {
        'ERROR': '\033[31m',  # Red
        'WARNING': '\033[33m',  # Yellow
        'RESET': '\033[0m'
    }

    def format(self, record):
        level_color = self.COLORS.get(record.levelname, '')
        reset = self.COLORS['RESET']

        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_parts = []

        # æ·»åŠ agentå’Œanswerä¿¡æ¯
        if hasattr(record, 'agent_name') and record.agent_name:
            context_parts.append(f"Agent:{record.agent_name}")
        if hasattr(record, 'answer_name') and record.answer_name:
            context_parts.append(f"Answer:{record.answer_name}")
        if hasattr(record, 'node_id') and record.node_id:
            context_parts.append(f"Node:{record.node_id}")
        if hasattr(record, 'op_id') and record.op_id:
            context_parts.append(f"Op:{record.op_id}")

        context_str = " | ".join(context_parts)
        context_prefix = f"[{context_str}] " if context_str else ""

        return f"{level_color}[{record.levelname}]{reset} {context_prefix}{record.getMessage()}"


class HumanReadableFormatter(logging.Formatter):
    """äººç±»å¯è¯»çš„æ–‡ä»¶æ—¥å¿—æ ¼å¼ï¼Œä¿ç•™emoji"""

    def format(self, record):
        # æ—¶é—´æˆ³ - ç²¾ç¡®åˆ°ç§’
        timestamp = self.formatTime(record, '%Y-%m-%d %H:%M:%S')

        # åŸºæœ¬ä¿¡æ¯ - åªåœ¨é‡è¦çº§åˆ«æ˜¾ç¤ºlevel
        level_prefix = ""
        if record.levelname in ['ERROR', 'WARNING']:
            level_prefix = f"[{record.levelname}] "

        base_info = f"[{timestamp}] {level_prefix}{record.getMessage()}"

        # æ·»åŠ ç»“æ„åŒ–ä¿¡æ¯
        extras = []
        skip_fields = {
            'name', 'msg', 'args', 'levelname', 'levelno', 'pathname',
            'filename', 'module', 'lineno', 'funcName', 'created',
            'msecs', 'relativeCreated', 'thread', 'threadName',
            'processName', 'process', 'getMessage', 'exc_info',
            'exc_text', 'stack_info', 'message'
        }

        for key, value in record.__dict__.items():
            if key not in skip_fields and value is not None:
                # ç‰¹æ®Šå¤„ç†ä¸€äº›å­—æ®µçš„æ˜¾ç¤º
                if key == 'final_score' and isinstance(value, (int, float)):
                    extras.append(f"score={value}")
                elif key == 'agent_name':
                    extras.append(f"agent={value}")
                elif key == 'node_id':
                    extras.append(f"node={value}")
                elif key == 'op_id':
                    extras.append(f"op={value}")
                else:
                    extras.append(f"{key}={value}")

        if extras:
            base_info += f" | {' | '.join(extras)}"

        return base_info


class CompactJsonFormatter(jsonlogger.JsonFormatter):
    """ç²¾ç®€çš„ JSON æ ¼å¼åŒ–å™¨ï¼Œç§»é™¤å†—ä½™å­—æ®µ"""

    def add_fields(self, log_record, record, message_dict):
        super().add_fields(log_record, record, message_dict)

        # ç§»é™¤ä¸éœ€è¦çš„å­—æ®µ
        fields_to_remove = ['name', 'levelname']
        for field in fields_to_remove:
            log_record.pop(field, None)

        # ç®€åŒ–æ—¶é—´æ ¼å¼åˆ°ç§’
        if 'asctime' in log_record:
            try:
                asctime = log_record['asctime']
                if ',' in asctime:
                    log_record['asctime'] = asctime.split(',')[0]
            except:
                pass


def _get_shared_error_handler() -> StreamHandler:
    """è·å–æˆ–åˆ›å»ºå…¨å±€å…±äº«çš„é”™è¯¯handler"""
    global _shared_error_handler

    with _handler_lock:
        if _shared_error_handler is None:
            _shared_error_handler = StreamHandler(sys.stderr)  # ä½¿ç”¨stderræ˜¾ç¤ºé”™è¯¯
            _shared_error_handler.setFormatter(ErrorWithContextFormatter())
            _shared_error_handler.setLevel(logging.ERROR)  # åªæ˜¾ç¤ºERRORçº§åˆ«

    return _shared_error_handler


def create_logger(
        lgr_nm: str,
        log_folder: str,
        enable_console: bool = True,
        file_format: Literal["jsonl", "readable", "both"] = "both",
        enable_shared_errors: bool = False  # æ–°å¢å‚æ•°
) -> tuple[Logger, str]:
    """
    åˆ›å»ºç‹¬ç«‹çš„loggerå®ä¾‹ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼

    Args:
        lgr_nm: loggeråç§°
        log_folder: æ—¥å¿—æ–‡ä»¶å¤¹
        enable_console: æ˜¯å¦å¯ç”¨æ§åˆ¶å°è¾“å‡º
        file_format: æ–‡ä»¶æ—¥å¿—æ ¼å¼
        enable_shared_errors: æ˜¯å¦å°†ERRORçº§åˆ«çš„æ—¥å¿—è¾“å‡ºåˆ°å…±äº«çš„terminal

    Returns:
        (loggerå®ä¾‹, æ—¶é—´æˆ³)
    """
    if not os.path.exists(log_folder):
        os.makedirs(log_folder)

    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆ›å»ºå”¯ä¸€çš„loggeråç§°ï¼Œé¿å…é‡å¤
    unique_logger_name = f"{lgr_nm}_{current_time}_{id(log_folder)}"

    # æ£€æŸ¥loggeræ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™å…ˆæ¸…ç†
    existing_logger = logging.getLogger(unique_logger_name)
    if existing_logger.handlers:
        for handler in existing_logger.handlers[:]:
            existing_logger.removeHandler(handler)
            handler.close()

    # åˆ›å»ºæ–°çš„logger
    new_logger = logging.getLogger(unique_logger_name)
    new_logger.setLevel(logging.DEBUG)
    new_logger.propagate = False

    # æ–‡ä»¶handlers
    if file_format in ["jsonl", "both"]:
        # JSON Lines æ ¼å¼
        jsonl_file = os.path.join(log_folder, f"{current_time}_{lgr_nm}.jsonl")
        jsonl_handler = TimedRotatingFileHandler(
            jsonl_file,
            when="D",
            backupCount=14,
            encoding="utf-8"
        )
        jsonl_formatter = CompactJsonFormatter('%(asctime)s %(message)s')
        jsonl_handler.setFormatter(jsonl_formatter)
        jsonl_handler.setLevel(logging.DEBUG)
        new_logger.addHandler(jsonl_handler)

    if file_format in ["readable", "both"]:
        # äººç±»å¯è¯»æ ¼å¼
        readable_file = os.path.join(log_folder, f"{current_time}_{lgr_nm}.log")
        readable_handler = TimedRotatingFileHandler(
            readable_file,
            when="D",
            backupCount=14,
            encoding="utf-8"
        )
        readable_formatter = HumanReadableFormatter()
        readable_handler.setFormatter(readable_formatter)
        readable_handler.setLevel(logging.DEBUG)
        new_logger.addHandler(readable_handler)

    # æ§åˆ¶å°handler - ä½¿ç”¨å½©è‰²ç»“æ„åŒ–æ ¼å¼
    if enable_console:
        console_handler = StreamHandler(sys.stdout)
        console_handler.setFormatter(ColoredStructuredFormatter())
        console_handler.setLevel(logging.INFO)
        new_logger.addHandler(console_handler)

    # å…±äº«é”™è¯¯handler - ç”¨äºåœ¨å¹¶è¡Œæ‰§è¡Œæ—¶æ˜¾ç¤ºé”™è¯¯
    if enable_shared_errors:
        shared_error_handler = _get_shared_error_handler()
        new_logger.addHandler(shared_error_handler)

    return new_logger, current_time


def create_sub_logger(parent_logger: Logger, sub_name: str) -> Logger:
    """
    Create sublogger based on parent logger, inherit parent logger's handlers
    Used to create hierarchical logs within the same evaluation
    """
    parent_name = parent_logger.name
    sub_logger_name = f"{parent_name}.{sub_name}"

    sub_logger = logging.getLogger(sub_logger_name)
    sub_logger.setLevel(parent_logger.level)
    sub_logger.propagate = True  # Allow propagation to parent logger

    return sub_logger


def cleanup_logger(logger: Logger) -> None:
    """æ¸…ç†loggerçš„æ‰€æœ‰handlersï¼ˆä½†ä¸æ¸…ç†å…±äº«çš„é”™è¯¯handlerï¼‰"""
    global _shared_error_handler

    for handler in logger.handlers[:]:
        # ä¸è¦æ¸…ç†å…±äº«çš„é”™è¯¯handler
        if handler is not _shared_error_handler:
            logger.removeHandler(handler)
            handler.close()
        else:
            logger.removeHandler(handler)  # åªç§»é™¤ï¼Œä¸å…³é—­


def cleanup_shared_error_handler():
    """åœ¨ç¨‹åºç»“æŸæ—¶æ¸…ç†å…±äº«çš„é”™è¯¯handler"""
    global _shared_error_handler

    with _handler_lock:
        if _shared_error_handler is not None:
            _shared_error_handler.close()
            _shared_error_handler = None


# ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜
"""
åœ¨ evaluation runner ä¸­çš„ä½¿ç”¨æ–¹æ³•ï¼š

1. ä¸»logger - æ­£å¸¸çš„æ§åˆ¶å°è¾“å‡ºï¼š
   main_logger, timestamp = create_logger("main_task", log_folder, enable_console=True)

2. å„ä¸ªanswerçš„logger - é”™è¯¯ä¼šæ˜¾ç¤ºåˆ°terminalï¼š
   logger, timestamp = create_logger(
       log_tag, 
       str(log_dir), 
       enable_console=False,  # ä¸å¯ç”¨å¸¸è§„æ§åˆ¶å°è¾“å‡º
       enable_shared_errors=True  # å¯ç”¨å…±äº«é”™è¯¯è¾“å‡º
   )

è¿™æ ·çš„æ•ˆæœï¼š
- ä¸»è¦çš„è¿›åº¦ä¿¡æ¯åœ¨ä¸»terminalæ˜¾ç¤º
- å„ä¸ªanswerçš„ERRORçº§åˆ«ä¿¡æ¯ä¹Ÿä¼šæ˜¾ç¤ºåˆ°terminalï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
- æ‰€æœ‰è¯¦ç»†æ—¥å¿—ä»ç„¶ä¿å­˜åˆ°å„è‡ªçš„æ–‡ä»¶ä¸­

ç»ˆç«¯è¾“å‡ºç¤ºä¾‹ï¼š
ğŸš€ Starting concurrent evaluation of 10 answers
ğŸ‘‰ Processing human/answer_1.md
[ERROR] [Agent:human | Answer:answer_1.md | Node:price_check] Failed to verify price claim
ğŸ‘‰ Processing openai_deep_research/answer_1.md
âœ… Successfully evaluated human/answer_1.md
"""